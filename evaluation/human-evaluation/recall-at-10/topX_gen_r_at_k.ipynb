{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "__author__ = \"Juho Leinonen\"\n",
    "__copyright__ = \"Copyright (c) 2020, Aalto Speech Research\"\n",
    "# Notebook to create the csv file.\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Combine messages following each other if by same sender\n",
    "# Filter messages that are too short\n",
    "# Pick from each group enough chats that there is total at least x good messages for r@k\n",
    "# From those chats pick those x messages\n",
    "# Other chats than those will go to train set\n",
    "# For both sets create r@k set where every good line has 4 false choices from all sentences of that group\n",
    "# that have at least 10 characters.\n",
    "# save to csv. format sentence Â¤ correct next sentence | false 1 | false 2 | ... | false k-1\n",
    "#####################################\n",
    "# ###################\n",
    "######### Setting up the pipeline ######################\n",
    "########################################################\n",
    "\n",
    "# So the run can be replicated\n",
    "SEED = 415\n",
    "random.seed(SEED)\n",
    "\n",
    "# Read the chat conversations to a dataframe\n",
    "conversations_file = \"../data/clean_data/finchat_200331.csv\"    \n",
    "conversations_df = pd.read_csv(conversations_file, sep=',', engine='python')\n",
    "\n",
    "# Read the metadata to a dataframe:\n",
    "metadata_file = \"../data/clean_data/meta_data_200304.csv\"    \n",
    "metadata_df = pd.read_csv(metadata_file, sep=',', engine='python')\n",
    "\n",
    "# For the for loop\n",
    "max_chat_id = conversations_df['CHAT_ID'].max()\n",
    "\n",
    "# CHOOSE THESE\n",
    "desired_lines_from_staff = 100\n",
    "desired_lines_from_students = 100\n",
    "desired_lines_from_highschoolers = 100\n",
    "desired_lines = [desired_lines_from_staff, desired_lines_from_students, desired_lines_from_highschoolers]\n",
    "\n",
    "# CHOOSE THIS\n",
    "recall_k = 10\n",
    "\n",
    "print_testing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "############################ CODE ############################\n",
    "\n",
    "df_columns = ['CHAT_ID', 'SPEAKER_ID', 'TEXT', 'GOOD']\n",
    "df_compressed = pd.DataFrame(columns=df_columns)\n",
    "df_train = pd.DataFrame(columns=df_columns)\n",
    "df_eval  = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "# Indices of the chats of different groups, choose only those that were not offtopic\n",
    "staff_chat_idx = metadata_df.loc[(metadata_df['GROUP'] == 1) & (metadata_df['OFFTOPIC'] == '0')]['CHAT_ID'].unique().tolist()\n",
    "students_chat_idx = metadata_df.loc[(metadata_df['GROUP'] == 2) & (metadata_df['OFFTOPIC'] == '0')]['CHAT_ID'].unique().tolist()\n",
    "highschoolers_chat_idx = metadata_df.loc[(metadata_df['GROUP'] == 3) & (metadata_df['OFFTOPIC'] == '0')]['CHAT_ID'].unique().tolist()\n",
    "\n",
    "group_idxs = [staff_chat_idx, students_chat_idx, highschoolers_chat_idx]\n",
    "\n",
    "# Using a dictionary instead of list in case there are empty chat IDs\n",
    "chat_lengths = {}\n",
    "eval_chat_idx = []\n",
    "train_chat_idx = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Going over all chats compressing them and marking good sentences\n",
    "for i in range(max_chat_id + 1):\n",
    "    \n",
    "    df_current_chat = conversations_df.loc[conversations_df['CHAT_ID'] == i][['CHAT_ID','SPEAKER_ID', 'TEXT']]\n",
    "    \n",
    "    # In case the chat ID skips one or more\n",
    "    if df_current_chat.empty:\n",
    "        continue\n",
    "    \n",
    "    # Checking whether one person sent multiple messages in a row, then combines them with a \" <MS> \".\n",
    "    # Adds the chat ID back.\n",
    "    adj_check = (df_current_chat['SPEAKER_ID'] != df_current_chat['SPEAKER_ID'].shift()).cumsum()\n",
    "    \n",
    "    df_current_chat = df_current_chat.groupby(['SPEAKER_ID', adj_check], as_index=False, sort=False).agg({'TEXT' : ' <MS> '.join})   \n",
    "    df_current_chat.insert(0, 'CHAT_ID', i)\n",
    "\n",
    "    # Filter short sentences.\n",
    "    mask_of_good_sentences = list(df_current_chat['TEXT'].str.len() > 10)\n",
    "    \n",
    "    # If the next sentence is too short, and cannot be used, \n",
    "    # then it cannot be something to predict from this sentence\n",
    "    # range 0 to the second last\n",
    "    for j in range(len(mask_of_good_sentences) - 1):        \n",
    "        if not mask_of_good_sentences[j+1]:\n",
    "            mask_of_good_sentences[j] = False\n",
    "            \n",
    "    # The last sentence does not have a next sentence\n",
    "    mask_of_good_sentences[-1] = False\n",
    "    \n",
    "    df_current_chat[\"GOOD\"] = mask_of_good_sentences\n",
    "    \n",
    "    # How many good sentences and total sentences\n",
    "    chat_lengths[i] = [df_current_chat.shape[0], sum(mask_of_good_sentences)]\n",
    "    \n",
    "    # Add to new compressed version of dataframe\n",
    "    df_compressed = df_compressed.append(df_current_chat, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Randomly collect chat indices with total lines of at least x from group\n",
    "def collect_eval_lines(idxs, amount_of_lines):\n",
    "    chat_idxs = []\n",
    "    total_lines = 0\n",
    "    idx_to_pick_from = [idx for idx in idxs]\n",
    "    while total_lines < amount_of_lines:\n",
    "        current_chat_id = random.choice(idx_to_pick_from)\n",
    "        \n",
    "        idx_to_pick_from.remove(current_chat_id)\n",
    "        chat_idxs.append(current_chat_id)\n",
    "        \n",
    "        total_lines += chat_lengths[current_chat_id][1]\n",
    "    return chat_idxs\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(group_idxs)):\n",
    "    chats_to_eval = collect_eval_lines(group_idxs[i], desired_lines[i])\n",
    "    \n",
    "    # Needs to be done chat by chat, otherwise orders them by index.\n",
    "    # e.g., the chats picked were 30, 30, 30, 80. So everything from the first\n",
    "    # three and then 10 from fourth, if all using isin(iterator) they would be\n",
    "    # maybe 30, 80, 30, 30.\n",
    "    sentences_so_far = 0\n",
    "    for chat in chats_to_eval[:-1]:  \n",
    "        df_to_eval = df_compressed.loc[df_compressed['CHAT_ID'] == chat]\n",
    "        df_eval = df_eval.append(df_to_eval, ignore_index=True)\n",
    "        sentences_so_far += chat_lengths[chat][1]\n",
    "        eval_chat_idx.append(chat)\n",
    "           \n",
    "    df_to_eval = df_compressed.loc[df_compressed['CHAT_ID'] == chats_to_eval[-1]]\n",
    "    \n",
    "    rows_to_pick_from_last = 0\n",
    "    for index, row in df_to_eval.iterrows():\n",
    "        rows_to_pick_from_last += 1\n",
    "        if row['GOOD']:\n",
    "            sentences_so_far += 1\n",
    "            if sentences_so_far == 100:\n",
    "                break\n",
    "    df_to_eval = df_to_eval.head(rows_to_pick_from_last + 1) #There is always a sentence after GOOD\n",
    "    \n",
    "    # Since the last line was split between chats it cannot be used\n",
    "    df_to_eval.iat[-1, -1] = False\n",
    "    df_eval = df_eval.append(df_to_eval, ignore_index=True)\n",
    "    eval_chat_idx.append(chats_to_eval[-1])\n",
    "    \n",
    "  \n",
    "df_to_train = df_compressed.loc[~df_compressed['CHAT_ID'].isin(eval_chat_idx)]\n",
    "df_train = df_train.append(df_to_train, ignore_index=True) # Easier way to reset index I think\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation set topics\n",
      "food          12\n",
      "sports         8\n",
      "music          6\n",
      "literature     5\n",
      "tv             4\n",
      "traveling      4\n",
      "Name: TOPIC, dtype: int64\n",
      "Whole set topics\n",
      "Staff\n",
      "sports        22\n",
      "traveling     14\n",
      "movies        14\n",
      "music          8\n",
      "literature     8\n",
      "food           8\n",
      "tv             6\n",
      "Name: TOPIC, dtype: int64\n",
      "Students\n",
      "food         10\n",
      "traveling    10\n",
      "Name: TOPIC, dtype: int64\n",
      "Highschoolers\n",
      "sports        15\n",
      "tv            12\n",
      "literature    10\n",
      "Name: TOPIC, dtype: int64\n",
      "lines in evaluation set\n",
      "420\n",
      "lines in train set\n",
      "2020\n",
      "Lines eval + train\n",
      "2440\n",
      "Lines in compressed set\n",
      "2466\n",
      "eval chats\n",
      "{19, 20, 21, 22, 24, 31, 57, 61, 66, 70, 72, 78, 87, 88, 89, 90, 91, 92, 95}\n",
      "train chats\n",
      "{4, 6, 7, 9, 10, 13, 14, 15, 16, 17, 18, 23, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 58, 59, 60, 62, 63, 64, 65, 67, 68, 69, 71, 73, 74, 75, 76, 77, 79, 80, 81, 83, 84, 86, 93, 94, 96}\n",
      "eval and train intersection\n",
      "set()\n",
      "chats not in eval or train\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "# Get topics of evaluation set\n",
    "if print_testing:\n",
    "    eval_set_topics = metadata_df.loc[metadata_df['CHAT_ID'].isin(eval_chat_idx)]['TOPIC'].value_counts()\n",
    "    print(\"Evaluation set topics\")\n",
    "    print(eval_set_topics)\n",
    "    \n",
    "    print(\"Whole set topics\")\n",
    "    print(\"Staff\")\n",
    "    print(metadata_df.loc[metadata_df['CHAT_ID'].isin(group_idxs[0])]['TOPIC'].value_counts())\n",
    "    print(\"Students\")\n",
    "    print(metadata_df.loc[metadata_df['CHAT_ID'].isin(group_idxs[1])]['TOPIC'].value_counts())\n",
    "    print(\"Highschoolers\")\n",
    "    print(metadata_df.loc[metadata_df['CHAT_ID'].isin(group_idxs[2])]['TOPIC'].value_counts())\n",
    "    \n",
    "    #Check some values\n",
    "    print(\"lines in evaluation set\")\n",
    "    print(len(df_eval))\n",
    "    print(\"lines in train set\")\n",
    "    print(len(df_train))\n",
    "    print(\"Lines eval + train\")\n",
    "    print(len(df_eval) + len(df_train))\n",
    "    print(\"Lines in compressed set\")\n",
    "    print(len(df_compressed))\n",
    "    \n",
    "    # No chat accidentally in train and set both or neither\n",
    "    eval_chats_set = set(df_eval['CHAT_ID'].unique())\n",
    "    train_chats_set = set(df_train['CHAT_ID'].unique())\n",
    "    compressed_chats_set = set(df_compressed['CHAT_ID'].unique())\n",
    "    \n",
    "    print(\"eval chats\")\n",
    "    print(eval_chats_set)\n",
    "    \n",
    "    print(\"train chats\")\n",
    "    print(train_chats_set)\n",
    "    \n",
    "    print(\"eval and train intersection\")\n",
    "    print(eval_chats_set.intersection(train_chats_set))\n",
    "    \n",
    "    print(\"chats not in eval or train\")\n",
    "    print(compressed_chats_set.difference(eval_chats_set.union(train_chats_set)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Going over train and eval separately and randomly choosing for GOOD sentences\n",
    "# The incorrect will be picked from not short sentences (except current and next).\n",
    "# Will then be saved to a list with correct next sentence as first item.\n",
    "\n",
    "# Dictionary used so the file can be written correctly as train or eval\n",
    "df_dict = {\"train\" : df_train, \"eval\" : df_eval}\n",
    "\n",
    "#If current sentence has BAD value True skipped. And not chosen randomly for false isNextSentence\n",
    "for df_name, df_set in df_dict.items():\n",
    "    df_set_len = df_set.shape[0]\n",
    "    next_sentence_list = []\n",
    "    indices_of_long_sentences = df_set.index[df_set[\"TEXT\"].str.len() > 10].tolist()\n",
    "    \n",
    "    for index, row in df_set.iterrows():\n",
    "        if row['GOOD']:\n",
    "                already_picked = []\n",
    "                bad_indices = {index, index + 1}\n",
    "                choice_sentences = []\n",
    "                \n",
    "                # First picking k - 1 false choices, no sentence more than once.\n",
    "                for _ in range(recall_k - 1):\n",
    "                    bad_indices.update(already_picked)\n",
    "                    false_sentence_index = random.choice([idx for idx in indices_of_long_sentences if idx not in bad_indices])\n",
    "                    \n",
    "                    choice_sentences.append(df_set.at[false_sentence_index, \"TEXT\"])\n",
    "                    already_picked.append(false_sentence_index)\n",
    "                \n",
    "                choice_sentences.insert(0, df_set.at[index + 1, \"TEXT\"])\n",
    "                \n",
    "                next_sentence_list.append(\" | \".join(choice_sentences))\n",
    "        else:\n",
    "            next_sentence_list.append(\"PASS\")\n",
    "    \n",
    "    # Make the next sentence list into a dataframe so it will be easier to concatenate\n",
    "    df_next_sentence = pd.DataFrame(next_sentence_list, columns=[\"CHOICE_SENTENCES\"])\n",
    "    \n",
    "    # Concatenate, take only the good sentences, reset the index and save with only necessary columns\n",
    "    # TODO find a unique separator, now Â¤ works.\n",
    "    df_new_set = pd.concat([df_set, df_next_sentence], axis=1, sort=False).query(\"GOOD\").reset_index()[[\"TEXT\", \"CHOICE_SENTENCES\"]]\n",
    "    csv_name = \"../data/\" + df_name + \"_topX_recall_at_\" + str(recall_k) + \"_\" + str(SEED) + \".csv\"\n",
    "    df_new_set.to_csv(csv_name, index=False, sep='Â¤')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
