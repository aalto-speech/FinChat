{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "source": "# Imports\nimport pandas as pd\nimport random\n\n# Combine messages following each other if by same sender\n# Filter messages that are too short\n# Pick from each group enough chats that there is total at least x good messages for r@k\n# From those chats pick those x messages\n# Other chats than those will go to train set\n# For both sets create r@k set where every good line has 4 false choices from all sentences of that group\n# that have at least 10 characters.\n# save to csv. format sentence Â¤ correct next sentence | false 1 | false 2 | ... | false k-1\n#####################################\n# ###################\n######### Setting up the pipeline ######################\n########################################################\n\n# So the run can be replicated\nSEED \u003d 415\nrandom.seed(SEED)\n\n# Read the chat conversations to a dataframe\nconversations_file \u003d \"../data/clean_data/finchat_200331.csv\"    \nconversations_df \u003d pd.read_csv(conversations_file, sep\u003d\u0027,\u0027, engine\u003d\u0027python\u0027)\n\n# Read the metadata to a dataframe:\nmetadata_file \u003d \"../data/clean_data/meta_data_200304.csv\"    \nmetadata_df \u003d pd.read_csv(metadata_file, sep\u003d\u0027,\u0027, engine\u003d\u0027python\u0027)\n\n# For the for loop\nmax_chat_id \u003d conversations_df[\u0027CHAT_ID\u0027].max()\n\n# CHOOSE THESE\ndesired_lines_from_staff \u003d 100\ndesired_lines_from_students \u003d 100\ndesired_lines_from_highschoolers \u003d 100\ndesired_lines \u003d [desired_lines_from_staff, desired_lines_from_students, desired_lines_from_highschoolers]\n\n# CHOOSE THIS\nrecall_k \u003d 10\n\nprint_testing \u003d True",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "############################ CODE ############################\n\ndf_columns \u003d [\u0027CHAT_ID\u0027, \u0027SPEAKER_ID\u0027, \u0027TEXT\u0027, \u0027GOOD\u0027]\ndf_compressed \u003d pd.DataFrame(columns\u003ddf_columns)\ndf_train \u003d pd.DataFrame(columns\u003ddf_columns)\ndf_eval  \u003d pd.DataFrame(columns\u003ddf_columns)\n\n# Indices of the chats of different groups, choose only those that were not offtopic\nstaff_chat_idx \u003d metadata_df.loc[(metadata_df[\u0027GROUP\u0027] \u003d\u003d 1) \u0026 (metadata_df[\u0027OFFTOPIC\u0027] \u003d\u003d \u00270\u0027)][\u0027CHAT_ID\u0027].unique().tolist()\nstudents_chat_idx \u003d metadata_df.loc[(metadata_df[\u0027GROUP\u0027] \u003d\u003d 2) \u0026 (metadata_df[\u0027OFFTOPIC\u0027] \u003d\u003d \u00270\u0027)][\u0027CHAT_ID\u0027].unique().tolist()\nhighschoolers_chat_idx \u003d metadata_df.loc[(metadata_df[\u0027GROUP\u0027] \u003d\u003d 3) \u0026 (metadata_df[\u0027OFFTOPIC\u0027] \u003d\u003d \u00270\u0027)][\u0027CHAT_ID\u0027].unique().tolist()\n\ngroup_idxs \u003d [staff_chat_idx, students_chat_idx, highschoolers_chat_idx]\n\n# Using a dictionary instead of list in case there are empty chat IDs\nchat_lengths \u003d {}\neval_chat_idx \u003d []\ntrain_chat_idx \u003d []"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Going over all chats compressing them and marking good sentences\n",
        "for i in range(max_chat_id + 1):\n",
        "    \n",
        "    df_current_chat \u003d conversations_df.loc[conversations_df[\u0027CHAT_ID\u0027] \u003d\u003d i][[\u0027CHAT_ID\u0027,\u0027SPEAKER_ID\u0027, \u0027TEXT\u0027]]\n",
        "    \n",
        "    # In case the chat ID skips one or more\n",
        "    if df_current_chat.empty:\n",
        "        continue\n",
        "    \n",
        "    # Checking whether one person sent multiple messages in a row, then combines them with a \" \u003cMS\u003e \".\n",
        "    # Adds the chat ID back.\n",
        "    adj_check \u003d (df_current_chat[\u0027SPEAKER_ID\u0027] !\u003d df_current_chat[\u0027SPEAKER_ID\u0027].shift()).cumsum()\n",
        "    \n",
        "    df_current_chat \u003d df_current_chat.groupby([\u0027SPEAKER_ID\u0027, adj_check], as_index\u003dFalse, sort\u003dFalse).agg({\u0027TEXT\u0027 : \u0027 \u003cMS\u003e \u0027.join})   \n",
        "    df_current_chat.insert(0, \u0027CHAT_ID\u0027, i)\n",
        "\n",
        "    # Filter short sentences.\n",
        "    mask_of_good_sentences \u003d list(df_current_chat[\u0027TEXT\u0027].str.len() \u003e 10)\n",
        "    \n",
        "    # If the next sentence is too short, and cannot be used, \n",
        "    # then it cannot be something to predict from this sentence\n",
        "    # range 0 to the second last\n",
        "    for j in range(len(mask_of_good_sentences) - 1):        \n",
        "        if not mask_of_good_sentences[j+1]:\n",
        "            mask_of_good_sentences[j] \u003d False\n",
        "            \n",
        "    # The last sentence does not have a next sentence\n",
        "    mask_of_good_sentences[-1] \u003d False\n",
        "    \n",
        "    df_current_chat[\"GOOD\"] \u003d mask_of_good_sentences\n",
        "    \n",
        "    # How many good sentences and total sentences\n",
        "    chat_lengths[i] \u003d [df_current_chat.shape[0], sum(mask_of_good_sentences)]\n",
        "    \n",
        "    # Add to new compressed version of dataframe\n",
        "    df_compressed \u003d df_compressed.append(df_current_chat, ignore_index\u003dTrue)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Randomly collect chat indices with total lines of at least x from group\n",
        "def collect_eval_lines(idxs, amount_of_lines):\n",
        "    chat_idxs \u003d []\n",
        "    total_lines \u003d 0\n",
        "    idx_to_pick_from \u003d [idx for idx in idxs]\n",
        "    while total_lines \u003c amount_of_lines:\n",
        "        current_chat_id \u003d random.choice(idx_to_pick_from)\n",
        "        \n",
        "        idx_to_pick_from.remove(current_chat_id)\n",
        "        chat_idxs.append(current_chat_id)\n",
        "        \n",
        "        total_lines +\u003d chat_lengths[current_chat_id][1]\n",
        "    return chat_idxs\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(group_idxs)):\n",
        "    chats_to_eval \u003d collect_eval_lines(group_idxs[i], desired_lines[i])\n",
        "    \n",
        "    # Needs to be done chat by chat, otherwise orders them by index.\n",
        "    # e.g., the chats picked were 30, 30, 30, 80. So everything from the first\n",
        "    # three and then 10 from fourth, if all using isin(iterator) they would be\n",
        "    # maybe 30, 80, 30, 30.\n",
        "    sentences_so_far \u003d 0\n",
        "    for chat in chats_to_eval[:-1]:  \n",
        "        df_to_eval \u003d df_compressed.loc[df_compressed[\u0027CHAT_ID\u0027] \u003d\u003d chat]\n",
        "        df_eval \u003d df_eval.append(df_to_eval, ignore_index\u003dTrue)\n",
        "        sentences_so_far +\u003d chat_lengths[chat][1]\n",
        "        eval_chat_idx.append(chat)\n",
        "           \n",
        "    df_to_eval \u003d df_compressed.loc[df_compressed[\u0027CHAT_ID\u0027] \u003d\u003d chats_to_eval[-1]]\n",
        "    \n",
        "    rows_to_pick_from_last \u003d 0\n",
        "    for index, row in df_to_eval.iterrows():\n",
        "        rows_to_pick_from_last +\u003d 1\n",
        "        if row[\u0027GOOD\u0027]:\n",
        "            sentences_so_far +\u003d 1\n",
        "            if sentences_so_far \u003d\u003d 100:\n",
        "                break\n",
        "    df_to_eval \u003d df_to_eval.head(rows_to_pick_from_last + 1) #There is always a sentence after GOOD\n",
        "    \n",
        "    # Since the last line was split between chats it cannot be used\n",
        "    df_to_eval.iat[-1, -1] \u003d False\n",
        "    df_eval \u003d df_eval.append(df_to_eval, ignore_index\u003dTrue)\n",
        "    eval_chat_idx.append(chats_to_eval[-1])\n",
        "    \n",
        "  \n",
        "df_to_train \u003d df_compressed.loc[~df_compressed[\u0027CHAT_ID\u0027].isin(eval_chat_idx)]\n",
        "df_train \u003d df_train.append(df_to_train, ignore_index\u003dTrue) # Easier way to reset index I think\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Evaluation set topics\nfood          12\nsports         8\nmusic          6\nliterature     5\ntv             4\ntraveling      4\nName: TOPIC, dtype: int64\nWhole set topics\nStaff\nsports        22\ntraveling     14\nmovies        14\nmusic          8\nliterature     8\nfood           8\ntv             6\nName: TOPIC, dtype: int64\nStudents\nfood         10\ntraveling    10\nName: TOPIC, dtype: int64\nHighschoolers\nsports        15\ntv            12\nliterature    10\nName: TOPIC, dtype: int64\nlines in evaluation set\n420\nlines in train set\n2020\nLines eval + train\n2440\nLines in compressed set\n2466\neval chats\n{19, 20, 21, 22, 24, 31, 57, 61, 66, 70, 72, 78, 87, 88, 89, 90, 91, 92, 95}\ntrain chats\n{4, 6, 7, 9, 10, 13, 14, 15, 16, 17, 18, 23, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 58, 59, 60, 62, 63, 64, 65, 67, 68, 69, 71, 73, 74, 75, 76, 77, 79, 80, 81, 83, 84, 86, 93, 94, 96}\neval and train intersection\nset()\nchats not in eval or train\nset()\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# Get topics of evaluation set\nif print_testing:\n    eval_set_topics \u003d metadata_df.loc[metadata_df[\u0027CHAT_ID\u0027].isin(eval_chat_idx)][\u0027TOPIC\u0027].value_counts()\n    print(\"Evaluation set topics\")\n    print(eval_set_topics)\n    \n    print(\"Whole set topics\")\n    print(\"Staff\")\n    print(metadata_df.loc[metadata_df[\u0027CHAT_ID\u0027].isin(group_idxs[0])][\u0027TOPIC\u0027].value_counts())\n    print(\"Students\")\n    print(metadata_df.loc[metadata_df[\u0027CHAT_ID\u0027].isin(group_idxs[1])][\u0027TOPIC\u0027].value_counts())\n    print(\"Highschoolers\")\n    print(metadata_df.loc[metadata_df[\u0027CHAT_ID\u0027].isin(group_idxs[2])][\u0027TOPIC\u0027].value_counts())\n    \n    #Check some values\n    print(\"lines in evaluation set\")\n    print(len(df_eval))\n    print(\"lines in train set\")\n    print(len(df_train))\n    print(\"Lines eval + train\")\n    print(len(df_eval) + len(df_train))\n    print(\"Lines in compressed set\")\n    print(len(df_compressed))\n    \n    # No chat accidentally in train and set both or neither\n    eval_chats_set \u003d set(df_eval[\u0027CHAT_ID\u0027].unique())\n    train_chats_set \u003d set(df_train[\u0027CHAT_ID\u0027].unique())\n    compressed_chats_set \u003d set(df_compressed[\u0027CHAT_ID\u0027].unique())\n    \n    print(\"eval chats\")\n    print(eval_chats_set)\n    \n    print(\"train chats\")\n    print(train_chats_set)\n    \n    print(\"eval and train intersection\")\n    print(eval_chats_set.intersection(train_chats_set))\n    \n    print(\"chats not in eval or train\")\n    print(compressed_chats_set.difference(eval_chats_set.union(train_chats_set)))\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "# Going over train and eval separately and randomly choosing for GOOD sentences\n# The incorrect will be picked from not short sentences (except current and next).\n# Will then be saved to a list with correct next sentence as first item.\n\n# Dictionary used so the file can be written correctly as train or eval\ndf_dict \u003d {\"train\" : df_train, \"eval\" : df_eval}\n\n#If current sentence has BAD value True skipped. And not chosen randomly for false isNextSentence\nfor df_name, df_set in df_dict.items():\n    df_set_len \u003d df_set.shape[0]\n    next_sentence_list \u003d []\n    indices_of_long_sentences \u003d df_set.index[df_set[\"TEXT\"].str.len() \u003e 10].tolist()\n    \n    for index, row in df_set.iterrows():\n        if row[\u0027GOOD\u0027]:\n                already_picked \u003d []\n                bad_indices \u003d {index, index + 1}\n                choice_sentences \u003d []\n                \n                # First picking k - 1 false choices, no sentence more than once.\n                for _ in range(recall_k - 1):\n                    bad_indices.update(already_picked)\n                    false_sentence_index \u003d random.choice([idx for idx in indices_of_long_sentences if idx not in bad_indices])\n                    \n                    choice_sentences.append(df_set.at[false_sentence_index, \"TEXT\"])\n                    already_picked.append(false_sentence_index)\n                \n                choice_sentences.insert(0, df_set.at[index + 1, \"TEXT\"])\n                \n                next_sentence_list.append(\" | \".join(choice_sentences))\n        else:\n            next_sentence_list.append(\"PASS\")\n    \n    # Make the next sentence list into a dataframe so it will be easier to concatenate\n    df_next_sentence \u003d pd.DataFrame(next_sentence_list, columns\u003d[\"CHOICE_SENTENCES\"])\n    \n    # Concatenate, take only the good sentences, reset the index and save with only necessary columns\n    # TODO find a unique separator, now Â¤ works.\n    df_new_set \u003d pd.concat([df_set, df_next_sentence], axis\u003d1, sort\u003dFalse).query(\"GOOD\").reset_index()[[\"TEXT\", \"CHOICE_SENTENCES\"]]\n    csv_name \u003d \"../data/\" + df_name + \"_topX_recall_at_\" + str(recall_k) + \"_\" + str(SEED) + \".csv\"\n    df_new_set.to_csv(csv_name, index\u003dFalse, sep\u003d\u0027Â¤\u0027)\n\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}